{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6W9YXCkuwVV/GnyBNyatr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhushanmandava/LLM_Concepts/blob/main/Sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MyyPjXvE1vV",
        "outputId": "453f442b-f235-4ebf-c5c5-a465e13086c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test vocabulary: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog']\n",
            "Initial logits: [ 2.   1.5  1.   0.5  0.  -0.5 -1.  -1.5]\n",
            "\n",
            "Sampling with different parameters:\n",
            "\n",
            "Test 1: Default parameters (temperature=0.7, no top-k/p filtering)\n",
            "Samples: [np.str_('the'), np.str_('the'), np.str_('brown'), np.str_('quick'), np.str_('the')]\n",
            "\n",
            "Test 2: High temperature (temperature=2.0)\n",
            "Samples: [np.str_('jumps'), np.str_('the'), np.str_('the'), np.str_('the'), np.str_('quick')]\n",
            "\n",
            "Test 3: Low temperature (temperature=0.2)\n",
            "Samples: [np.str_('the'), np.str_('the'), np.str_('the'), np.str_('the'), np.str_('the')]\n",
            "\n",
            "Test 4: Top-k filtering (top_k=3)\n",
            "Samples: [np.str_('the'), np.str_('the'), np.str_('quick'), np.str_('quick'), np.str_('the')]\n",
            "\n",
            "Test 5: Top-p filtering (top_p=0.9)\n",
            "Samples: [np.str_('fox'), np.str_('fox'), np.str_('fox'), np.str_('dog'), np.str_('fox')]\n",
            "\n",
            "Test 6: Combined filtering (temperature=0.5, top_k=3, top_p=0.9)\n",
            "Samples: [np.str_('quick'), np.str_('brown'), np.str_('quick'), np.str_('quick'), np.str_('quick')]\n",
            "\n",
            "Error handling examples:\n",
            "Expected error: some vocab is missng their logits\n",
            "Expected error: temperature must be grater than zero\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def validate_inputs(logits,vocabulary,temperature,top_p,top_k):\n",
        "  if len(vocabulary)!= len(logits):\n",
        "    raise ValueError(\"some vocab is missng their logits\")\n",
        "  if temperature<=0:\n",
        "    raise ValueError(\"temperature must be grater than zero\")\n",
        "  if top_k <0 or top_k>len(logits):\n",
        "    raise ValueError(\"top_k values must be b/w 0 and logits \")\n",
        "  if not 0<top_p<=1:\n",
        "    raise ValueError(\"top_p values must be b/w 0 and 1 its.is probabilty threshold\")\n",
        "def get_token_count(perv_tokens,vocabulary):\n",
        "  token_counts = {}\n",
        "  if perv_tokens is not None:\n",
        "    for token in perv_tokens:\n",
        "      if token in vocabulary:\n",
        "        idx =vocabulary.idx(token)\n",
        "        token_counts[idx] = token_counts.get(idx,0)+1\n",
        "  return token_counts\n",
        "\n",
        "def apply_precence_penality(logits , token_counts , presence_penality):#this is for not letting repeat the token again\n",
        "  if presence_penality!=0.0:\n",
        "    for idx in token_counts:\n",
        "      logits[idx]-=presence_penality\n",
        "  return logits\n",
        "def apply_frequency_penality(logits,token_counts, frequency_penality):\n",
        "  if frequency_penality!=0.0:\n",
        "    for idx,count in token_counts.items():\n",
        "      logits[idx]-=frequency_penality*count\n",
        "  return logits\n",
        "def apply_temperature(logits,temperature):\n",
        "  if temperature!=1.0:\n",
        "    logits = logits/temperature\n",
        "  return logits - np.max(logits)\n",
        "def apply_top_k_filtering(logits , top_k,min_tokens=1):\n",
        "  if top_k>0:\n",
        "    indices_to_remove = np.argsort(logits)[:-min_tokens]\n",
        "    indices_to_keep = np.argsort(logits)[-top_k:]\n",
        "    for idx in indices_to_remove:\n",
        "      if idx not in indices_to_keep:\n",
        "        logits[idx]=-np.inf\n",
        "  return logits\n",
        "def apply_top_p_filtering(logits, top_p, min_tokens=1):\n",
        "    if top_p <= 1.0:\n",
        "        probs = np.exp(logits)\n",
        "        probs = probs / np.sum(probs)\n",
        "        sorted_indices = np.argsort(probs)[::-1]\n",
        "        sorted_probs = probs[sorted_indices]\n",
        "        cumulative_probs = np.cumsum(sorted_probs)\n",
        "\n",
        "        sorted_indices_to_remove = sorted_indices[cumulative_probs < top_p]\n",
        "\n",
        "        if len(sorted_indices_to_remove) > len(sorted_indices) - min_tokens:\n",
        "            sorted_indices_to_remove = sorted_indices_to_remove[\n",
        "                :len(sorted_indices) - min_tokens\n",
        "            ]\n",
        "\n",
        "        logits[sorted_indices_to_remove] = float('-inf')\n",
        "    return logits\n",
        "\n",
        "def convert_to_probabilites(logits):\n",
        "  probs = np.exp(logits)\n",
        "  probs = probs/np.sum(probs)\n",
        "  return probs\n",
        "def sample_token(logits,vocabulary,temperature=0.7,top_k=0,top_p=0.1,repetition_penalty=1.0, presence_penalty=0.0, frequency_penalty=0.0,\n",
        "                prev_tokens=None):\n",
        "  validate_inputs(logits,vocabulary,temperature,top_p,top_k)\n",
        "  logits = np.array(logits,dtype=np.float64)\n",
        "  token_counts = get_token_count(prev_tokens,vocabulary)\n",
        "  logits = apply_precence_penality(logits,token_counts,presence_penalty)\n",
        "  logits = apply_frequency_penality(logits,token_counts,frequency_penalty)\n",
        "  logits = apply_temperature(logits,temperature)\n",
        "  logits = apply_top_k_filtering(logits,top_k)\n",
        "  logits = apply_top_p_filtering(logits,top_p)\n",
        "  probs = convert_to_probabilites(logits)\n",
        "  token = np.random.choice(vocabulary,p=probs)\n",
        "  return token\n",
        "if __name__ == \"__main__\":\n",
        "    vocabulary = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\"]\n",
        "    logits = np.array([2.0, 1.5, 1.0, 0.5, 0.0, -0.5, -1.0, -1.5])\n",
        "\n",
        "    print(\"Test vocabulary:\", vocabulary)\n",
        "    print(\"Initial logits:\", logits)\n",
        "    print(\"\\nSampling with different parameters:\")\n",
        "\n",
        "    print(\"\\nTest 1: Default parameters (temperature=0.7, no top-k/p filtering)\")\n",
        "    samples = [sample_token(logits.copy(), vocabulary) for _ in range(5)]\n",
        "    print(\"Samples:\", samples)\n",
        "\n",
        "    print(\"\\nTest 2: High temperature (temperature=2.0)\")\n",
        "    samples = [sample_token(logits.copy(), vocabulary, temperature=2.0) for _ in range(5)]\n",
        "    print(\"Samples:\", samples)\n",
        "\n",
        "    print(\"\\nTest 3: Low temperature (temperature=0.2)\")\n",
        "    samples = [sample_token(logits.copy(), vocabulary, temperature=0.2) for _ in range(5)]\n",
        "    print(\"Samples:\", samples)\n",
        "\n",
        "    print(\"\\nTest 4: Top-k filtering (top_k=3)\")\n",
        "    samples = [sample_token(logits.copy(), vocabulary, top_k=3) for _ in range(5)]\n",
        "    print(\"Samples:\", samples)\n",
        "\n",
        "    print(\"\\nTest 5: Top-p filtering (top_p=0.9)\")\n",
        "    samples = [sample_token(logits.copy(), vocabulary, top_p=0.9) for _ in range(5)]\n",
        "    print(\"Samples:\", samples)\n",
        "\n",
        "    print(\"\\nTest 6: Combined filtering (temperature=0.5, top_k=3, top_p=0.9)\")\n",
        "    samples = [sample_token(logits.copy(), vocabulary, temperature=0.5, top_k=3, top_p=0.9)\n",
        "              for _ in range(5)]\n",
        "    print(\"Samples:\", samples)\n",
        "\n",
        "    print(\"\\nError handling examples:\")\n",
        "    try:\n",
        "        sample_token(logits[:5], vocabulary)\n",
        "    except ValueError as e:\n",
        "        print(\"Expected error:\", e)\n",
        "\n",
        "    try:\n",
        "        sample_token(logits, vocabulary, temperature=0)\n",
        "    except ValueError as e:\n",
        "        print(\"Expected error:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "icO7KdIaHjOA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}